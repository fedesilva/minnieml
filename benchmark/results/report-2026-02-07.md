# MML benchmark report
Date: 2026-02-07

Previous report: [2026-01-14](report-2026-01-14.md)

## Changes since last report

Compiler changes that could affect codegen:

- **`noalias` attributes** (6490e16): Allocating return values and consuming pointer params now emit `noalias` in the LLVM IR, enabling LLVM alias analysis optimizations for loop-heavy array code.
- **`inline` keyword + attribute groups** (3710c2b): New `inline fn`/`inline op` syntax. All functions now go through attribute group infrastructure (`#0` for normal, `#1` for `inlinehint`). This changes how every function is annotated in the IR, even when `inline` is not used.
- **Ownership/borrow enforcement** (10f5993, e2e40c2, 237abe1, df9c3bb): Compile-time only — borrow escape via return, last-use validation for consuming params, ban on partial application of consuming-param functions. No codegen impact expected.


## Preliminary notes

### Unsafe array access

Most MML benchmarks (Sieve, Quicksort, Matrix Multiplication, N-Queens) use `unsafe_ar_int_*` intrinsics for array access. These map directly to LLVM load/store instructions without bounds checking, like raw pointer access in C.

This gives MML an advantage over Rust and Go, which enforce bounds checking by default (Go's BCE should help, but see the matmul results below). The `unsafe` usage puts MML closer to C for these micro-benchmarks.

The MML type system will eventually provide safety guarantees that produce equivalent code. Where possible, bounds checks will be verified at compile time using effects tracking, affine types, and GDP (Ghosts of Departed Proofs).

These benchmarks are a baseline. They keep us honest about performance as we add safety features.

### Data types and memory pressure

The generalizing typechecker is still under development, so benchmarks use a specialized `IntArray` rather than polymorphic arrays.

All benchmarks use `int64`, the only integer type `IntArray` fully supports. This keeps comparison fair across languages, but also increases memory subsystem pressure compared to benchmarks that might use 8-bit or 32-bit integers.

## Sieve of Eratosthenes

Finds all primes up to 1,000,000 using a mutable array of 64-bit integers.

- `sieve-mml`: Tail recursion for loops, `unsafe_ar_int_*` for array access.
- `sieve-c`: `malloc` + `while` loops, with branchless summation in the counting phase.
- `sieve-rs`: Idiomatic Rust, `Vec<i64>` + `while` loops, bounds checking enabled.
- `sieve-go`: Idiomatic Go, slices + `for` loops.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/sieve-c` | 7.3 ± 0.8 | 6.3 | 9.3 | 1.00 |
| `bin/sieve-mml` | 7.4 ± 0.7 | 6.5 | 9.1 | 1.02 ± 0.14 |
| `bin/sieve-rs` | 7.4 ± 0.6 | 6.6 | 8.7 | 1.02 ± 0.13 |
| `bin/sieve-go` | 9.7 ± 0.8 | 8.5 | 13.7 | 1.33 ± 0.18 |

MML ties C again (1.02x, well within noise). Also ties Rust this time. Go trails by ~33%, same as January.

Previous results (Jan 14):

| Command | Mean [ms] | Relative |
|:---|---:|---:|
| `bin/sieve-c` | 9.3 ± 0.4 | 1.00 |
| `bin/sieve-mml` | 9.4 ± 0.5 | 1.01 ± 0.07 |

## Quicksort

In-place quicksort on 1,000,000 integers filled via LCG.

- `quicksort-mml`: Tail recursion for partitioning, recursive calls for sorting.
- `quicksort-c`: Standard C, `while` loops.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `benchmark/bin/quicksort-c` | 71.5 ± 4.3 | 65.4 | 94.1 | 1.00 |
| `benchmark/bin/quicksort-mml` | 81.4 ± 4.5 | 75.3 | 94.4 | 1.14 ± 0.09 |

MML trails C by ~14%, down from a ~1% lead in January. Some of the gap is noise at this timescale, but there's a small real regression here.

Previous results (Jan 14):

| Command | Mean [ms] | Relative |
|:---|---:|---:|
| `bin/quicksort-mml` | 108.2 ± 0.7 | 1.00 |
| `bin/quicksort-c` | 109.8 ± 0.8 | 1.01 ± 0.01 |

## Matrix multiplication

Naive multiplication of two 500x500 matrices of 64-bit integers.

- `matmul-mml`: O(N^3) triple loop.
- `matmul-opt-mml`: Loop-interchanged (i-k-j) for cache locality.
- `matmul-c`: O(N^3) triple loop, strided access on B.
- `matmul-opt-c`: Loop-interchanged (i-k-j), standard C.
- `matmul-restricted-c`: Same as `matmul-opt-c` with `restrict` (no real gain).
- `matmul-go`: O(N^3) triple loop, slices.
- `matmul-opt-go`: Loop-interchanged (i-k-j), idiomatic Go, no manual BCE.
- `matmul-bce-go`: Explicit BCE hints (`_ = A[size-1]`).

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/matmul-restricted-c` | 47.1 ± 4.4 | 41.0 | 64.3 | 1.00 |
| `bin/matmul-opt-c` | 47.6 ± 3.8 | 41.3 | 66.8 | 1.01 ± 0.12 |
| `bin/matmul-opt-mml` | 47.7 ± 1.7 | 43.5 | 53.1 | 1.01 ± 0.10 |
| `bin/matmul-opt-go` | 94.8 ± 2.4 | 90.9 | 102.1 | 2.01 ± 0.20 |
| `bin/matmul-go` | 176.5 ± 6.8 | 169.6 | 195.6 | 3.75 ± 0.38 |
| `bin/matmul-bce-go` | 249.2 ± 6.5 | 241.3 | 271.6 | 5.29 ± 0.52 |
| `bin/matmul-c` | 253.2 ± 11.1 | 238.2 | 290.8 | 5.37 ± 0.56 |
| `bin/matmul-mml` | 253.2 ± 6.4 | 241.3 | 274.2 | 5.37 ± 0.52 |

Optimized MML reaches C parity: 47.7ms vs 47.1ms (restricted) / 47.6ms (opt). The 0.1-0.6ms difference is noise. This is due to the `noalias` attribute on `ar_int_new`'s return value, which lets LLVM prove the output array doesn't alias the inputs and optimize the inner loop.

Previous optimized results (Jan 14):

| Command | Mean [ms] | Relative |
|:---|---:|---:|
| `bin/matmul-restricted-c` | 44.4 ± 2.4 | 1.00 |
| `bin/matmul-opt-c` | 44.5 ± 3.0 | 1.00 ± 0.09 |
| `bin/matmul-opt-mml` | 70.2 ± 4.4 | 1.58 ± 0.13 |

Naive MML is now 253.2ms, matching naive C exactly (also 253.2ms). In January, naive MML was 99.6ms — 2.5x faster than C's 244.5ms. That old advantage was suspicious and is now gone. The `noalias` attribute may have changed LLVM's optimization strategy for the strided access pattern: with alias info, LLVM may no longer speculatively reorder the naive loop in the same way it did before.

## N-Queens

N=12, counting all 14,200 valid solutions. Backtracking with recursion and an array for board state.

- `nqueens-mml`: Tail-call optimization for the inner `is_safe` loops.
- `nqueens-c`: Standard recursive backtracking.
- `nqueens-go`: Standard recursive backtracking. *(new)*

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/nqueens-c` | 91.7 ± 4.2 | 86.6 | 101.1 | 1.00 |
| `bin/nqueens-mml` | 144.4 ± 3.4 | 140.2 | 154.5 | 1.57 ± 0.08 |
| `bin/nqueens-go` | 145.9 ± 4.9 | 139.6 | 156.8 | 1.59 ± 0.09 |

MML trails C by ~57%, improved from ~65% in January. MML slightly edges out Go. Still the weakest benchmark -- not clear yet whether the gap is call overhead, the small-array access pattern, or something else.

Previous results (Jan 14):

| Command | Mean [ms] | Relative |
|:---|---:|---:|
| `bin/nqueens-c` | 131.8 ± 0.4 | 1.00 |
| `bin/nqueens-mml` | 217.8 ± 0.5 | 1.65 ± 0.01 |

## Euclidean extended GCD

Extended Euclidean algorithm in a loop, RSA-style modular exponentiation. Integer-arithmetic and recursion heavy.

- `euclidean-ext-mml`: Recursive.
- `euclidean-ext-c`: Iterative.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/euclidean-ext-c` | 5.0 ± 0.4 | 4.1 | 6.9 | 1.00 |
| `bin/euclidean-ext-mml` | 5.4 ± 0.4 | 4.4 | 6.6 | 1.08 ± 0.12 |

MML trails C by ~8%, improved from ~20% in January. The gap is now within noise given the error bars.

Previous results (Jan 14):

| Command | Mean [ms] | Relative |
|:---|---:|---:|
| `bin/euclidean-ext-c` | 7.2 ± 0.2 | 1.00 |
| `bin/euclidean-ext-mml` | 8.6 ± 0.2 | 1.20 ± 0.05 |

## Ackermann

A(3, 10). Pure recursion stress test, all about function call overhead.

- `ackermann-mml`: Recursive.
- `ackermann-c`: Recursive.
- `ackermann-c-chacho`: Alternative C implementation. *(new)*
- `ackermann-rs`: Recursive.
- `ackermann-go`: Recursive.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/ackermann-c` | 120.9 ± 5.4 | 113.7 | 134.1 | 1.00 |
| `bin/ackermann-c-chacho` | 122.4 ± 5.8 | 117.2 | 142.7 | 1.01 ± 0.07 |
| `bin/ackermann-mml` | 147.7 ± 4.6 | 143.5 | 162.5 | 1.22 ± 0.07 |
| `bin/ackermann-rs` | 176.9 ± 8.0 | 166.3 | 196.8 | 1.46 ± 0.09 |
| `bin/ackermann-go` | 216.8 ± 7.0 | 209.7 | 236.4 | 1.79 ± 0.10 |

MML trails C by ~22%, a notable regression from the dead-heat in January. Ackermann is pure call overhead with no arrays, so `noalias` has no effect here. The likely cause is the attribute group infrastructure added for the `inline` keyword: every function now references an attribute group (`#0`), which may change how LLVM processes function attributes. The two C implementations (`ackermann-c` and `ackermann-c-chacho`) produce nearly identical results, confirming the benchmark is stable.

Previous results (Jan 14):

| Command | Mean [ms] | Relative |
|:---|---:|---:|
| `bin/ackermann-c` | 112.5 ± 7.1 | 1.00 |
| `bin/ackermann-mml` | 112.6 ± 5.3 | 1.00 ± 0.08 |
| `bin/ackermann-rs` | 132.6 ± 6.1 | 1.18 ± 0.09 |
| `bin/ackermann-go` | 199.2 ± 8.9 | 1.77 ± 0.14 |

## MML self-benchmark: Sieve (optimization levels)

Sieve compiled at O0-O3, with and without MML's tail call optimization (TCO). At `-O0`, TCO gives a 4x speedup (7.1ms vs 28.6ms). At `-O1` and above, LLVM's own tail call elimination closes the gap.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/sieve-mml-O0-tco` | 7.1 ± 0.4 | 6.6 | 9.0 | 1.00 |
| `bin/sieve-mml-O0-no-tco` | 28.6 ± 3.0 | 25.2 | 37.6 | 4.00 ± 0.49 |
| `bin/sieve-mml-O1-tco` | 7.9 ± 0.9 | 6.8 | 10.2 | 1.10 ± 0.15 |
| `bin/sieve-mml-O1-no-tco` | 8.4 ± 1.5 | 6.7 | 13.9 | 1.17 ± 0.23 |
| `bin/sieve-mml-O2-tco` | 8.5 ± 1.0 | 6.7 | 11.3 | 1.18 ± 0.16 |
| `bin/sieve-mml-O2-no-tco` | 7.7 ± 0.9 | 6.6 | 9.9 | 1.08 ± 0.14 |
| `bin/sieve-mml-O3-tco` | 7.9 ± 0.7 | 6.8 | 10.1 | 1.10 ± 0.12 |
| `bin/sieve-mml-O3-no-tco` | 8.1 ± 1.0 | 6.8 | 10.9 | 1.13 ± 0.16 |

Results are consistent with January. The O0-TCO advantage is slightly larger (4.0x vs 3.6x). At O1+, all variants cluster within noise.

## MML self-benchmark: matmul naive (optimization levels)

Naive O(N^3) matmul across optimization levels.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/matmul-mml-O0-tco` | 110.1 ± 4.5 | 103.0 | 122.1 | 1.03 ± 0.06 |
| `bin/matmul-mml-O0-no-tco` | 965.9 ± 35.7 | 934.9 | 1130.8 | 9.07 ± 0.53 |
| `bin/matmul-mml-O1-tco` | 106.5 ± 4.8 | 99.4 | 123.3 | 1.00 |
| `bin/matmul-mml-O1-no-tco` | 111.1 ± 5.1 | 102.7 | 130.6 | 1.04 ± 0.07 |
| `bin/matmul-mml-O2-tco` | 257.3 ± 8.9 | 242.4 | 274.4 | 2.42 ± 0.14 |
| `bin/matmul-mml-O2-no-tco` | 255.4 ± 7.8 | 241.2 | 273.9 | 2.40 ± 0.13 |
| `bin/matmul-mml-O3-tco` | 335.3 ± 126.6 | 241.8 | 867.4 | 3.15 ± 1.20 |
| `bin/matmul-mml-O3-no-tco` | 263.7 ± 4.5 | 259.3 | 282.8 | 2.48 ± 0.12 |

**Major regression at O2/O3.** In January, O2 ran at ~96ms and O3 at ~95ms. Now O2 is 257ms and O3 is 264-335ms — roughly 2.7x slower. O0 and O1 are in the same ballpark as before.

This is the same pattern as the naive matmul vs C regression: with `noalias` information available, LLVM's optimizer at `-O2`/`-O3` is making different (worse) choices for the naive strided-access loop. At `-O1`, the optimizer doesn't attempt the aggressive loop transforms that `noalias` enables, so it stays fast.

Previous results (Jan 14):

| Command | Mean [ms] | Relative |
|:---|---:|---:|
| `bin/matmul-mml-O2-tco` | 96.4 ± 10.2 | 1.02 ± 0.12 |
| `bin/matmul-mml-O2-no-tco` | 97.6 ± 10.7 | 1.03 ± 0.13 |
| `bin/matmul-mml-O3-tco` | 95.3 ± 19.4 | 1.01 ± 0.21 |
| `bin/matmul-mml-O3-no-tco` | 94.8 ± 5.8 | 1.00 |

## MML self-benchmark: matmul optimized (optimization levels)

Loop-interchanged matmul (i-k-j) across optimization levels.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/matmul-opt-mml-O0-tco` | 94.8 ± 5.5 | 86.4 | 115.4 | 2.02 ± 0.13 |
| `bin/matmul-opt-mml-O0-no-tco` | 918.8 ± 19.3 | 892.2 | 969.8 | 19.61 ± 0.66 |
| `bin/matmul-opt-mml-O1-tco` | 67.7 ± 3.4 | 62.5 | 79.8 | 1.45 ± 0.08 |
| `bin/matmul-opt-mml-O1-no-tco` | 83.4 ± 3.4 | 79.1 | 103.0 | 1.78 ± 0.09 |
| `bin/matmul-opt-mml-O2-tco` | 48.0 ± 2.2 | 45.6 | 60.6 | 1.02 ± 0.05 |
| `bin/matmul-opt-mml-O2-no-tco` | 48.8 ± 2.8 | 44.5 | 60.2 | 1.04 ± 0.07 |
| `bin/matmul-opt-mml-O3-tco` | 46.9 ± 1.2 | 44.7 | 49.2 | 1.00 |
| `bin/matmul-opt-mml-O3-no-tco` | 47.8 ± 2.6 | 44.0 | 59.9 | 1.02 ± 0.06 |

O2/O3 improved significantly thanks to `noalias`: 48ms vs 68ms in January (29% faster). O0-TCO regressed slightly (95ms vs 85ms in January). The O0-no-TCO penalty is slightly larger (919ms vs 852ms).

Previous results (Jan 14):

| Command | Mean [ms] | Relative |
|:---|---:|---:|
| `bin/matmul-opt-mml-O2-tco` | 68.3 ± 4.1 | 1.01 ± 0.09 |
| `bin/matmul-opt-mml-O3-tco` | 67.7 ± 4.6 | 1.00 ± 0.10 |
| `bin/matmul-opt-mml-O3-no-tco` | 67.4 ± 4.4 | 1.00 |

## Summary

### Wins

- **Optimized matmul reaches C parity** (47.7ms vs 47.1ms). The `noalias` attribute on `ar_int_new` returns lets LLVM prove arrays don't alias, enabling the same optimizations C gets with `restrict`. This was a 31% improvement from 70.2ms.
- **Euclidean improved** from 1.20x to 1.08x C — nearly within noise.
- **N-Queens improved** from 1.65x to 1.57x C. New Go entry shows MML slightly ahead of Go.

### Regressions

- **Ackermann: 1.00x to 1.22x C.** Pure call overhead, no arrays. Most likely caused by the attribute group infrastructure from the `inline` keyword. Every function now references an attribute group in the IR, which may affect LLVM's function-level optimization decisions. Worth investigating: compare IR diff with and without attribute groups.
- **Naive matmul: lost the 2.5x advantage over C.** Was 99.6ms vs C's 244.5ms (suspicious), now 253.2ms vs 253.2ms (equal). The `noalias` metadata changed LLVM's loop optimization strategy at O2/O3, which hurt the naive strided-access pattern while helping the cache-friendly optimized variant. The self-benchmark confirms: naive matmul at O2 went from 96ms to 257ms.
- **Quicksort: 0.99x to ~1.14x C.** Small real regression, though the relative number is inflated by noise at this timescale.

### To investigate

1. **Ackermann regression**: Emit IR with and without attribute groups, diff the LLVM output. If attribute groups are the cause, consider only emitting `#1` for `inline` functions and using inline attributes directly for others.
2. **Naive matmul O2/O3 regression**: Compare LLVM IR before and after `noalias`. The optimized variant benefits, but the naive variant suffers — LLVM may be attempting a loop interchange that's counterproductive for the strided access pattern.
3. **N-Queens**: Still 57% behind C. Profile to understand whether it's call overhead, array access patterns, or something else.
