# MML benchmark report
Date: 2026-01-14

## Preliminary notes

### Unsafe array access

Most MML benchmarks (Sieve, Quicksort, Matrix Multiplication, N-Queens) use `unsafe_ar_int_*` intrinsics for array access. These map directly to LLVM load/store instructions without bounds checking, like raw pointer access in C.

This gives MML an advantage over Rust and Go, which enforce bounds checking by default (Go's BCE should help, but see the matmul results below). The `unsafe` usage puts MML closer to C for these micro-benchmarks.

The MinnieML type system will eventually provide safety guarantees that produce equivalent code. Where possible, bounds checks will be verified at compile time using effects tracking, affine types, and GDP (Ghosts of Departed Proofs).

These benchmarks are a baseline. They keep us honest about performance as we add safety features.

### Data types and memory pressure

The generalizing typechecker is still under development, so benchmarks use a specialized `IntArray` rather than polymorphic arrays.

All benchmarks use `int64`, the only integer type `IntArray` fully supports. This keeps comparison fair across languages, but also increases memory subsystem pressure compared to benchmarks that might use 8-bit or 32-bit integers.

## Sieve of Eratosthenes

Finds all primes up to 1,000,000 using a mutable array of 64-bit integers.

- `sieve-mml`: Tail recursion for loops, `unsafe_ar_int_*` for array access.
- `sieve-c`: `malloc` + `while` loops, with branchless summation in the counting phase.
- `sieve-rs`: Idiomatic Rust, `Vec<i64>` + `while` loops, bounds checking enabled.
- `sieve-go`: Idiomatic Go, slices + `for` loops.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/sieve-c` | 9.3 ± 0.4 | 8.3 | 11.3 | 1.00 |
| `bin/sieve-mml` | 9.4 ± 0.5 | 8.3 | 11.7 | 1.01 ± 0.07 |
| `bin/sieve-rs` | 9.8 ± 0.4 | 9.0 | 11.9 | 1.06 ± 0.07 |
| `bin/sieve-go` | 12.3 ± 0.4 | 11.3 | 14.2 | 1.33 ± 0.08 |

MML is effectively tied with C -- the 0.1ms gap is noise (+-0.07). Beats Rust by ~4%, Go by ~33%.

## Quicksort

In-place quicksort on 1,000,000 integers filled via LCG.

- `quicksort-mml`: Tail recursion for partitioning, recursive calls for sorting.
- `quicksort-c`: Standard C, `while` loops.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/quicksort-c` | 109.8 ± 0.8 | 108.8 | 111.8 | 1.01 ± 0.01 |
| `bin/quicksort-mml` | 108.2 ± 0.7 | 107.2 | 110.1 | 1.00 |

MML wins by ~1.5%.

## Matrix multiplication

Naive multiplication of two 500x500 matrices of 64-bit integers.

- `matmul-mml`: O(N^3) triple loop.
- `matmul-opt-mml`: Loop-interchanged (i-k-j) for cache locality.
- `matmul-c`: O(N^3) triple loop, strided access on B.
- `matmul-opt-c`: Loop-interchanged (i-k-j), standard C.
- `matmul-restricted-c`: Same as `matmul-opt-c` with `restrict` (no real gain).
- `matmul-go`: O(N^3) triple loop, slices.
- `matmul-opt-go`: Loop-interchanged (i-k-j), idiomatic Go, no manual BCE.
- `matmul-bce-go`: Explicit BCE hints (`_ = A[size-1]`).

The `matmul-bce-go` variant actually performed worse than naive Go -- the manual BCE hints seem to have interfered with other compiler optimizations.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/matmul-restricted-c` | 44.4 ± 2.4 | 39.7 | 53.4 | 1.00 |
| `bin/matmul-opt-c` | 44.5 ± 3.0 | 37.8 | 49.3 | 1.00 ± 0.09 |
| `bin/matmul-opt-mml` | 70.2 ± 4.4 | 63.6 | 79.4 | 1.58 ± 0.13 |
| `bin/matmul-opt-go` | 94.3 ± 6.0 | 85.4 | 117.2 | 2.12 ± 0.18 |
| `bin/matmul-mml` | 99.6 ± 7.1 | 92.6 | 133.4 | 2.24 ± 0.20 |
| `bin/matmul-go` | 169.3 ± 8.8 | 158.6 | 194.9 | 3.81 ± 0.29 |
| `bin/matmul-c` | 244.5 ± 10.6 | 229.3 | 268.6 | 5.50 ± 0.38 |
| `bin/matmul-bce-go` | 254.5 ± 13.4 | 227.3 | 286.2 | 5.73 ± 0.43 |

Optimized MML is 58% behind the C versions but beats optimized Go by ~25%. The naive C version with its strided B access is dead last, which is a good reminder that cache locality matters more than language choice here.

## N-Queens

N=12, counting all 14,200 valid solutions. Backtracking with recursion and an array for board state.

- `nqueens-mml`: Tail-call optimization for the inner `is_safe` loops.
- `nqueens-c`: Standard recursive backtracking.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/nqueens-c` | 131.8 ± 0.4 | 131.3 | 132.6 | 1.00 |
| `bin/nqueens-mml` | 217.8 ± 0.5 | 217.1 | 219.5 | 1.65 ± 0.01 |

MML trails C by ~65%. This is the worst showing -- worth investigating what's different about the backtracking pattern.

## Euclidean extended GCD

Extended Euclidean algorithm in a loop, RSA-style modular exponentiation. Integer-arithmetic and recursion heavy.

- `euclidean-ext-mml`: Recursive.
- `euclidean-ext-c`: Iterative.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/euclidean-ext-c` | 7.2 ± 0.2 | 6.7 | 8.0 | 1.00 |
| `bin/euclidean-ext-mml` | 8.6 ± 0.2 | 8.2 | 9.0 | 1.20 ± 0.05 |

MML trails C by ~20%. mmlc converts the recursion to loops before LLVM even sees it, so the gap isn't call overhead. Worth investigating.

## Ackermann

A(3, 10). Pure recursion stress test, all about function call overhead.

- `ackermann-mml`: Recursive.
- `ackermann-c`: Recursive.
- `ackermann-rs`: Recursive.
- `ackermann-go`: Recursive.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/ackermann-c` | 112.5 ± 7.1 | 105.8 | 137.9 | 1.00 |
| `bin/ackermann-mml` | 112.6 ± 5.3 | 105.7 | 125.4 | 1.00 ± 0.08 |
| `bin/ackermann-rs` | 132.6 ± 6.1 | 125.0 | 144.9 | 1.18 ± 0.09 |
| `bin/ackermann-go` | 199.2 ± 8.9 | 188.9 | 223.6 | 1.77 ± 0.14 |

MML ties C (0.1ms difference, noise is +-5-7ms). Beats Rust by ~18% and Go by ~77%. Given that Ackermann is pure call overhead, this is a good sign for the calling convention.

## MML self-benchmark: Sieve (optimization levels)

Sieve compiled at O0-O3, with and without MML's tail call optimization (TCO). At `-O0`, TCO gives a 3.5x speedup (9.5ms vs 33.6ms). At `-O2` and above, LLVM's own tail call elimination closes the gap.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/sieve-mml-O0-tco` | 9.5 ± 0.4 | 8.4 | 11.5 | 1.02 ± 0.06 |
| `bin/sieve-mml-O0-no-tco` | 33.6 ± 2.1 | 31.1 | 40.1 | 3.61 ± 0.27 |
| `bin/sieve-mml-O1-tco` | 9.4 ± 0.4 | 8.3 | 11.3 | 1.01 ± 0.06 |
| `bin/sieve-mml-O1-no-tco` | 9.4 ± 0.4 | 8.5 | 10.9 | 1.01 ± 0.06 |
| `bin/sieve-mml-O2-tco` | 9.3 ± 0.4 | 8.5 | 11.5 | 1.00 ± 0.06 |
| `bin/sieve-mml-O2-no-tco` | 9.3 ± 0.4 | 8.4 | 11.2 | 1.00 |
| `bin/sieve-mml-O3-tco` | 9.3 ± 0.4 | 8.4 | 11.1 | 1.00 ± 0.06 |
| `bin/sieve-mml-O3-no-tco` | 9.4 ± 0.4 | 8.5 | 11.0 | 1.01 ± 0.06 |

## MML self-benchmark: matmul naive (optimization levels)

Naive O(N^3) matmul across optimization levels. At `-O0`, TCO gives a 7.9x speedup (159ms vs 1254ms). Without frontend loopification, the triple-recursive loop is brutal.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/matmul-mml-O0-tco` | 159.2 ± 7.2 | 155.7 | 256.3 | 1.68 ± 0.13 |
| `bin/matmul-mml-O0-no-tco` | 1254.5 ± 254.1 | 849.2 | 1636.2 | 13.23 ± 2.80 |
| `bin/matmul-mml-O1-tco` | 102.9 ± 8.1 | 89.3 | 168.9 | 1.08 ± 0.11 |
| `bin/matmul-mml-O1-no-tco` | 108.4 ± 18.9 | 90.5 | 307.1 | 1.14 ± 0.21 |
| `bin/matmul-mml-O2-tco` | 96.4 ± 10.2 | 86.7 | 191.2 | 1.02 ± 0.12 |
| `bin/matmul-mml-O2-no-tco` | 97.6 ± 10.7 | 86.3 | 174.7 | 1.03 ± 0.13 |
| `bin/matmul-mml-O3-tco` | 95.3 ± 19.4 | 86.0 | 462.8 | 1.01 ± 0.21 |
| `bin/matmul-mml-O3-no-tco` | 94.8 ± 5.8 | 86.4 | 125.7 | 1.00 |

## MML self-benchmark: matmul optimized (optimization levels)

Loop-interchanged matmul (i-k-j) across optimization levels. The biggest TCO win: at `-O0`, TCO gives a 10x speedup (85ms vs 852ms). Without it, this workload would likely stack overflow. With it, the recursion becomes a tight loop even before LLVM touches it.

| Command | Mean [ms] | Min [ms] | Max [ms] | Relative |
|:---|---:|---:|---:|---:|
| `bin/matmul-opt-mml-O0-tco` | 84.8 ± 4.0 | 78.8 | 106.3 | 1.26 ± 0.10 |
| `bin/matmul-opt-mml-O0-no-tco` | 851.6 ± 52.8 | 785.6 | 1164.1 | 12.64 ± 1.14 |
| `bin/matmul-opt-mml-O1-tco` | 67.6 ± 3.8 | 61.7 | 103.3 | 1.00 ± 0.09 |
| `bin/matmul-opt-mml-O1-no-tco` | 74.2 ± 5.5 | 66.6 | 123.1 | 1.10 ± 0.11 |
| `bin/matmul-opt-mml-O2-tco` | 68.3 ± 4.1 | 61.9 | 89.6 | 1.01 ± 0.09 |
| `bin/matmul-opt-mml-O2-no-tco` | 68.5 ± 4.0 | 61.8 | 86.9 | 1.02 ± 0.09 |
| `bin/matmul-opt-mml-O3-tco` | 67.7 ± 4.6 | 60.2 | 84.6 | 1.00 ± 0.10 |
| `bin/matmul-opt-mml-O3-no-tco` | 67.4 ± 4.4 | 60.1 | 90.1 | 1.00 |